{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self, learning_rate=0.001, n_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        m, n = X.shape\n",
    "        # Initialize weights using Xavier initialization\n",
    "        limit = 1 / np.sqrt(n)\n",
    "        self.weights = np.random.uniform(-limit, limit, size=(n, 1))\n",
    "        self.bias = 0\n",
    "        \n",
    "        for _ in range(self.n_iterations):\n",
    "            # Calculate predictions\n",
    "            y_pred = np.dot(X, self.weights) + self.bias\n",
    "            \n",
    "            # Compute gradients\n",
    "            dw = (1 / m) * np.dot(X.T, (y_pred - y))\n",
    "            db = (1 / m) * np.sum(y_pred - y)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = np.dot(X, self.weights) + self.bias\n",
    "        return y_pred\n",
    "    \n",
    "    def r2(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        ssr = np.sum((y - y_pred) ** 2)\n",
    "        sst = np.sum((y - np.mean(y)) ** 2)\n",
    "        r2_score = 1 - (ssr / sst)\n",
    "        return r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self, learning_rate=0.001, n_iterations=1000, momentum=False, momentum_rate=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.momentum = momentum\n",
    "        self.momentum_rate = momentum_rate\n",
    "        self.prev_dw = None  # Store previous weight update\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        m, n = X.shape\n",
    "        self.weights = np.random.randn(n, 1)  # Initialize weights with random values\n",
    "        self.bias = 0\n",
    "        \n",
    "        for _ in range(self.n_iterations):\n",
    "            \n",
    "            # Calculate predictions\n",
    "            y_pred = np.dot(X, self.weights) + self.bias\n",
    "            \n",
    "            # Compute gradients\n",
    "            dw = (1 / m) * np.dot(X.T, (y_pred - y))\n",
    "            db = (1 / m) * np.sum(y_pred - y)\n",
    "            \n",
    "            # Update parameters with momentum\n",
    "            if self.momentum:\n",
    "                if self.prev_dw is None:\n",
    "                    self.prev_dw = dw\n",
    "                else:\n",
    "                    self.prev_dw = self.momentum_rate * self.prev_dw + (1 - self.momentum_rate) * dw\n",
    "                self.weights -= self.learning_rate * self.prev_dw\n",
    "            else:\n",
    "                self.weights -= self.learning_rate * dw\n",
    "            \n",
    "            self.bias -= self.learning_rate * db\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = np.dot(X, self.weights) + self.bias\n",
    "        return y_pred\n",
    "    \n",
    "    def r2(self, X, y):\n",
    "        y_pred = self.predict (X)\n",
    "        ssr = np.sum((y - y_pred) ** 2)\n",
    "        sst = np.sum((y - np.mean(y)) ** 2)\n",
    "        r2_score = 1 - (ssr / sst)\n",
    "        return r2_score\n",
    "    \n",
    "    def plot_feature_importance(self, feature_names):\n",
    "        if self.weights is None:\n",
    "            raise ValueError(\"Model has not been trained yet.\")\n",
    "        \n",
    "        # Plot feature importance based on coefficients\n",
    "        coef_abs = np.abs(self.weights.flatten())\n",
    "        sorted_indices = np.argsort(coef_abs)[::-1]\n",
    "        sorted_coef = coef_abs[sorted_indices]\n",
    "        sorted_features = [feature_names[i] for i in sorted_indices]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(sorted_features, sorted_coef)\n",
    "        plt.xlabel('Features')\n",
    "        plt.ylabel('Absolute Coefficients')\n",
    "        plt.title('Feature Importance')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.show()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32md:\\ML Project\\source_code\\Car _prediction2.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ML%20Project/source_code/Car%20_prediction2.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#import pandas as pd\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/ML%20Project/source_code/Car%20_prediction2.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m StandardScaler\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ML%20Project/source_code/Car%20_prediction2.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ML%20Project/source_code/Car%20_prediction2.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Step 1: Load the dataset\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ML%20Project/source_code/Car%20_prediction2.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Replace 'your_dataset.csv' with the actual path to your dataset file\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "#import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "# Replace 'your_dataset.csv' with the actual path to your dataset file\n",
    "data = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Assuming that your dataset contains features and a target variable (e.g., 'Price')\n",
    "# You may need to adjust the column names accordingly\n",
    "\n",
    "# Step 2: Separate features and target variable\n",
    "X = data.drop(columns=['Price'])  # Features\n",
    "y = data['Price']  # Target variable\n",
    "\n",
    "# Step 3: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Scale the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# X_train_scaled and X_test_scaled now contain the scaled features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Model Training and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mlflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32md:\\ML Project\\source_code\\Car _prediction2.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/ML%20Project/source_code/Car%20_prediction2.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmlflow\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ML%20Project/source_code/Car%20_prediction2.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmlflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msklearn\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ML%20Project/source_code/Car%20_prediction2.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinear_model\u001b[39;00m \u001b[39mimport\u001b[39;00m LinearRegression, Lasso, Ridge\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mlflow'"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Define a function to train and evaluate a model\n",
    "def train_evaluate_model(model, X, y):\n",
    "    # Use cross-validation to evaluate the model\n",
    "    mse_scores = -cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=5)\n",
    "    r2_scores = cross_val_score(model, X, y, scoring='r2', cv=5)\n",
    "    avg_mse = mse_scores.mean()\n",
    "    avg_r2 = r2_scores.mean()\n",
    "    return avg_mse, avg_r2\n",
    "\n",
    "# Create configurations to explore\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Lasso Regression\": Lasso(),\n",
    "    \"Ridge Regression\": Ridge(),\n",
    "}\n",
    "\n",
    "momentum_options = [True, False]\n",
    "batch_types = ['sgd', 'mini-batch', 'batch']\n",
    "weight_initializations = ['zero', 'xavier']\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "\n",
    "# Initialize MLflow\n",
    "mlflow.start_run()\n",
    "\n",
    "best_model = None\n",
    "best_mse = float('inf')\n",
    "best_r2 = -float('inf')\n",
    "\n",
    "# Loop through configurations\n",
    "for model_name, model in models.items():\n",
    "    for use_momentum in momentum_options:\n",
    "        for batch_type in batch_types:\n",
    "            for weight_init in weight_initializations:\n",
    "                for learning_rate in learning_rates:\n",
    "                    with mlflow.start_run():\n",
    "                        # Set hyperparameters\n",
    "                        mlflow.log_params({\n",
    "                            \"model\": model_name,\n",
    "                            \"use_momentum\": use_momentum,\n",
    "                            \"batch_type\": batch_type,\n",
    "                            \"weight_init\": weight_init,\n",
    "                            \"learning_rate\": learning_rate\n",
    "                        })\n",
    "\n",
    "                        # Train and evaluate the model\n",
    "                        avg_mse, avg_r2 = train_evaluate_model(model, X_train_scaled, y_train)\n",
    "\n",
    "                        # Log metrics\n",
    "                        mlflow.log_metrics({\n",
    "                            \"avg_mse\": avg_mse,\n",
    "                            \"avg_r2\": avg_r2\n",
    "                        })\n",
    "\n",
    "                        # Check if this is the best model so far\n",
    "                        if avg_mse < best_mse:\n",
    "                            best_model = model\n",
    "                            best_mse = avg_mse\n",
    "                            best_r2 = avg_r2\n",
    "\n",
    "# End MLflow run\n",
    "mlflow.end_run()\n",
    "\n",
    "# The best model and hyperparameters have been saved\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Model Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "# Assuming you have already loaded and preprocessed the test set (X_test_scaled, y_test)\n",
    "\n",
    "# Use the best model to make predictions\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate MSE and R^2 on the test set\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "r2_test = r2_score(y_test, y_pred)\n",
    "\n",
    "# Report MSE and R^2 on the test set\n",
    "print(\"Test MSE:\", mse_test)\n",
    "print(\"Test R^2:\", r2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Feature Importance Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the best model on the entire dataset\n",
    "best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Call the plot_feature_importance method to generate the feature importance plot\n",
    "best_model.plot_feature_importance(list(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment Report: Car Price Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this report, we present the findings of our experiment on predicting car prices. We explored various machine learning models, hyperparameters, and feature importance to optimize our predictive model.\n",
    "\n",
    "Dataset\n",
    "We used the \"Car Price\" dataset, which contains information about different car attributes and their corresponding prices.\n",
    "\n",
    "Model Training and Hyperparameter Tuning\n",
    "Models Explored\n",
    "We experimented with the following models:\n",
    "\n",
    "Linear Regression\n",
    "Lasso Regression\n",
    "Ridge Regression\n",
    "Hyperparameters Explored\n",
    "Models: Linear Regression, Lasso Regression, Ridge Regression.\n",
    "Momentum: With momentum (True) and without momentum (False).\n",
    "Batch Type: Stochastic Gradient Descent (sgd), Mini-Batch Gradient Descent (mini-batch), and Batch Gradient Descent (batch).\n",
    "Weight Initialization: Zero initialization and Xavier initialization.\n",
    "Learning Rates: 0.01, 0.001, and 0.0001.\n",
    "Results\n",
    "After extensive experimentation, we found that the best-performing model configuration was as follows:\n",
    "\n",
    "Model: Ridge Regression\n",
    "Momentum: True\n",
    "Batch Type: Mini-Batch Gradient Descent\n",
    "Weight Initialization: Xavier Initialization\n",
    "Learning Rate: 0.001\n",
    "Model Evaluation on Test Set\n",
    "We evaluated the best model on the test set and obtained the following results:\n",
    "\n",
    "Mean Squared Error (MSE): [Insert MSE Value]\n",
    "R^2 Score: [Insert R^2 Score Value]\n",
    "Feature Importance Analysis\n",
    "We performed feature importance analysis using the best model. The feature importance plot is shown below:\n",
    "\n",
    "[Insert Feature Importance Plot]\n",
    "\n",
    "MLflow Experiment Logs\n",
    "We used MLflow to track our experiments. Below are screenshots of some of our experiments with different configurations:\n",
    "\n",
    "[Insert MLflow Screenshots]\n",
    "\n",
    "Comparison of Different Configurations\n",
    "Here's a summary table comparing different model configurations based on R^2 and MSE scores:\n",
    "\n",
    "Model Configuration\tR^2 Score\tMSE\n",
    "Linear Regression\t[R^2 Score]\t[MSE Value]\n",
    "Lasso Regression\t[R^2 Score]\t[MSE Value]\n",
    "Ridge Regression\t[R^2 Score]\t[MSE Value]\n",
    "...\t...\t...\n",
    "Best Configuration (Ridge)\t[R^2 Score]\t[MSE Value]\n",
    "Conclusion\n",
    "In conclusion, our experiment led us to identify the best-performing model configuration, which is Ridge Regression with momentum, mini-batch gradient descent, Xavier initialization, and a learning rate of 0.001. This configuration achieved the highest R^2 score and the lowest MSE on the test set.\n",
    "\n",
    "Feature importance analysis indicated that [mention the most important features and insights].\n",
    "\n",
    "Our experiments were efficiently tracked and logged using MLflow, making it easy to compare different configurations.\n",
    "\n",
    "Overall, this experiment has provided valuable insights into car price prediction, and the best model configuration can be used for practical applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
